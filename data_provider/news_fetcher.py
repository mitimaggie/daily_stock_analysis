# -*- coding: utf-8 -*-
"""
===================================
Akshare å…è´¹æ–°é—»é‡‡é›†å™¨
===================================

èŒè´£ï¼š
1. è°ƒç”¨ ak.stock_news_em() è·å–ä¸œæ–¹è´¢å¯Œä¸ªè‚¡æ–°é—»
2. æ ¼å¼åŒ–ä¸º SearchResult å¹¶å­˜å…¥ news_intel è¡¨
3. æ‰¹é‡é‡‡é›†æ‰€æœ‰è‡ªé€‰è‚¡æ–°é—»ï¼ˆä¾›åå°å®šæ—¶ä»»åŠ¡è°ƒç”¨ï¼‰

æ•°æ®æºï¼šä¸œæ–¹è´¢å¯Œï¼ˆå…è´¹ï¼ŒAè‚¡è¦†ç›–æœ€å…¨ï¼‰
"""

import logging
import time
import random
import hashlib
from datetime import datetime
from typing import List, Dict, Optional

logger = logging.getLogger(__name__)

# å†…å­˜çº§å»é‡ç¼“å­˜ï¼Œé¿å…åŒä¸€è¿›ç¨‹å†…çŸ­æ—¶é—´é‡å¤æ‹‰å–åŒä¸€åªè‚¡ç¥¨
_fetch_cooldown: Dict[str, float] = {}
_COOLDOWN_SECONDS = 600  # åŒä¸€åªè‚¡ç¥¨ 10 åˆ†é’Ÿå†…ä¸é‡å¤æ‹‰


def _parse_news_datetime(date_str: str) -> Optional[datetime]:
    """è§£æä¸œæ–¹è´¢å¯Œæ–°é—»çš„å‘å¸ƒæ—¶é—´å­—ç¬¦ä¸²"""
    if not date_str:
        return None
    for fmt in ("%Y-%m-%d %H:%M:%S", "%Y-%m-%d %H:%M", "%Y-%m-%d", "%Y%m%d %H:%M:%S"):
        try:
            return datetime.strptime(str(date_str).strip(), fmt)
        except (ValueError, TypeError):
            continue
    return None


def _build_url_key(code: str, title: str, source: str) -> str:
    """å½“æ–°é—»æ²¡æœ‰ URL æ—¶ï¼Œç”¨æ ‡é¢˜+æ¥æºç”Ÿæˆç¨³å®šçš„ä¼ª URLï¼ˆç”¨äºå»é‡ï¼‰"""
    raw = f"{code}:{title}:{source}"
    digest = hashlib.md5(raw.encode()).hexdigest()[:12]
    return f"akshare://news/{code}/{digest}"


def fetch_stock_news(code: str, limit: int = 20) -> List[Dict]:
    """
    è·å–å•åªè‚¡ç¥¨çš„ä¸œæ–¹è´¢å¯Œæ–°é—»

    Args:
        code: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ '002270'ï¼‰
        limit: æœ€å¤šè¿”å›æ¡æ•°

    Returns:
        ç»“æ„åŒ–æ–°é—»åˆ—è¡¨ [{"title", "snippet", "url", "source", "published_date"}, ...]
    """
    # å†·å´æ£€æŸ¥
    last_fetch = _fetch_cooldown.get(code, 0)
    if time.time() - last_fetch < _COOLDOWN_SECONDS:
        logger.debug(f"[{code}] æ–°é—»æŠ“å–å†·å´ä¸­ï¼Œè·³è¿‡")
        return []

    try:
        import akshare as ak
        df = ak.stock_news_em(symbol=code)
    except Exception as e:
        logger.warning(f"[{code}] Akshare æ–°é—»è·å–å¤±è´¥: {e}")
        return []

    if df is None or df.empty:
        logger.debug(f"[{code}] ä¸œæ–¹è´¢å¯Œæ— æ–°é—»æ•°æ®")
        _fetch_cooldown[code] = time.time()
        return []

    results = []
    # ä¸œæ–¹è´¢å¯Œè¿”å›çš„åˆ—åï¼šæ–°é—»æ ‡é¢˜, æ–°é—»å†…å®¹, å‘å¸ƒæ—¶é—´, æ–‡ç« æ¥æº, æ–°é—»é“¾æ¥
    for _, row in df.head(limit).iterrows():
        title = str(row.get("æ–°é—»æ ‡é¢˜", row.get("title", ""))).strip()
        snippet = str(row.get("æ–°é—»å†…å®¹", row.get("content", ""))).strip()
        pub_date = str(row.get("å‘å¸ƒæ—¶é—´", row.get("publish_time", "")))
        source = str(row.get("æ–‡ç« æ¥æº", row.get("source", "ä¸œæ–¹è´¢å¯Œ")))
        url = str(row.get("æ–°é—»é“¾æ¥", row.get("url", ""))).strip()

        if not title:
            continue
        if not url:
            url = _build_url_key(code, title, source)

        # æˆªæ–­è¿‡é•¿çš„æ‘˜è¦ï¼ˆèŠ‚çœ tokenï¼‰
        if len(snippet) > 500:
            snippet = snippet[:500] + "..."

        results.append({
            "title": title,
            "snippet": snippet,
            "url": url,
            "source": source,
            "published_date": pub_date,
        })

    _fetch_cooldown[code] = time.time()
    logger.info(f"ğŸ“° [{code}] ä¸œæ–¹è´¢å¯Œæ–°é—»æŠ“å–æˆåŠŸ: {len(results)} æ¡")
    return results


def save_news_to_db(code: str, stock_name: str, news_list: List[Dict]) -> int:
    """
    å°†æ–°é—»åˆ—è¡¨å­˜å…¥ news_intel è¡¨

    Args:
        code: è‚¡ç¥¨ä»£ç 
        stock_name: è‚¡ç¥¨åç§°
        news_list: fetch_stock_news è¿”å›çš„åˆ—è¡¨

    Returns:
        æ–°å¢å…¥åº“æ¡æ•°
    """
    if not news_list:
        return 0

    from src.storage import DatabaseManager, NewsIntel
    from sqlalchemy import select
    from sqlalchemy.exc import IntegrityError

    storage = DatabaseManager.get_instance()
    saved = 0
    with storage.get_session() as session:
        try:
            for item in news_list:
                url_key = item["url"]
                existing = session.execute(
                    select(NewsIntel).where(NewsIntel.url == url_key)
                ).scalar_one_or_none()

                if existing:
                    # å·²å­˜åœ¨ï¼šåˆ·æ–° fetched_atï¼ˆè¡¨ç¤ºä»ç„¶æ´»è·ƒï¼‰
                    existing.fetched_at = datetime.now()
                else:
                    try:
                        with session.begin_nested():
                            record = NewsIntel(
                                code=code,
                                name=stock_name,
                                dimension="èˆ†æƒ…",
                                query=f"akshare_news_{code}",
                                provider="akshare",
                                title=item["title"],
                                snippet=item["snippet"],
                                url=url_key,
                                source=item["source"],
                                published_date=_parse_news_datetime(item["published_date"]),
                                fetched_at=datetime.now(),
                                query_source="background",
                            )
                            session.add(record)
                        saved += 1
                    except IntegrityError:
                        pass
            session.commit()
        except Exception as e:
            session.rollback()
            logger.error(f"[{code}] æ–°é—»å…¥åº“å¤±è´¥: {e}")

    if saved > 0:
        logger.info(f"ğŸ’¾ [{code}] {stock_name} æ–°å¢ {saved} æ¡æ–°é—»å…¥åº“")
    return saved


def run_news_fetch_job(config) -> None:
    """
    åå°å®šæ—¶ä»»åŠ¡å…¥å£ï¼šä¸ºæ‰€æœ‰è‡ªé€‰è‚¡æŠ“å–æ–°é—»å¹¶å…¥åº“

    Args:
        config: Config å¯¹è±¡ï¼ˆéœ€è¦ stock_list å’Œ stock_namesï¼‰
    """
    config.refresh_stock_list()
    codes = config.stock_list
    if not codes:
        logger.warning("æœªé…ç½®è‡ªé€‰è‚¡åˆ—è¡¨ï¼Œè·³è¿‡æ–°é—»æŠ“å–")
        return

    stock_names = getattr(config, 'stock_names', {}) or {}
    logger.info(f"ğŸ“° å¼€å§‹åå°æ–°é—»æŠ“å–: {len(codes)} åªè‚¡ç¥¨")
    total_saved = 0

    for i, code in enumerate(codes):
        name = stock_names.get(code, code)
        try:
            news = fetch_stock_news(code)
            if news:
                saved = save_news_to_db(code, name, news)
                total_saved += saved
        except Exception as e:
            logger.warning(f"[{i+1}/{len(codes)}] {code} æ–°é—»æŠ“å–å¼‚å¸¸: {e}")

        # é˜²æ­¢è¯·æ±‚è¿‡å¿«è¢«å° IP
        if i < len(codes) - 1:
            sleep_time = random.uniform(2.0, 4.0)
            time.sleep(sleep_time)

    logger.info(f"ğŸ“° åå°æ–°é—»æŠ“å–å®Œæˆ: å…±æ–°å¢ {total_saved} æ¡æ–°é—»")
