# -*- coding: utf-8 -*-
import logging
import time
import random
import os
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError as FuturesTimeoutError

# === å¯¼å…¥æ•°æ®æ¨¡å— (ä¿æŒå¥å£®æ€§) ===
try:
    from data_provider import DataFetcherManager
except ImportError:
    try:
        from data_provider.base import DataFetcherManager
    except ImportError:
        # å°è¯•ä» src å¯¼å…¥
        from src.data_provider.base import DataFetcherManager

# å°è¯•å¯¼å…¥ F10 æ•°æ®è·å–å™¨
try:
    from data_provider.fundamental_fetcher import get_fundamental_data
except ImportError:
    def get_fundamental_data(code): return {}

# å°è¯•å¯¼å…¥ å¤§ç›˜ç›‘æ§ (Market Monitor) â€” ä¸ªè‚¡åˆ†ææ—¶ä½œä¸ºã€Œä»“ä½ä¸Šé™/å‰ç½®æ»¤ç½‘ã€
def _load_market_monitor():
    try:
        from data_provider.market_monitor import market_monitor
        return market_monitor
    except ImportError:
        try:
            import sys
            from pathlib import Path
            root = Path(__file__).resolve().parents[2]
            if str(root) not in sys.path:
                sys.path.insert(0, str(root))
            from data_provider.market_monitor import market_monitor
            return market_monitor
        except ImportError:
            return None

market_monitor = _load_market_monitor()

# === å†…éƒ¨æ¨¡å—å¯¼å…¥ ===
from src.stock_analyzer import StockTrendAnalyzer
from src.analyzer import GeminiAnalyzer, AnalysisResult
from src.notification import NotificationService
from src.storage import DatabaseManager  
from src.search_service import SearchService
from src.enums import ReportType

logger = logging.getLogger(__name__)

class StockAnalysisPipeline:
    """
    è‚¡ç¥¨åˆ†ææµæ°´çº¿ (æœ€ç»ˆå®Œæ•´ä¿®å¤ç‰ˆ)
    é€‚é… main.py çš„ config ä¼ å‚è°ƒç”¨æ–¹å¼ï¼ŒåŒ…å«ä¸¤é˜¶æ®µæ‰§è¡Œå’Œé˜²å°å·é€»è¾‘
    """
    def __init__(self, config, max_workers=3, query_id=None, query_source="cli", save_context_snapshot=True, source_message=None, **kwargs):
        """
        åˆå§‹åŒ– - ä¸¥æ ¼é€‚é… main.py çš„è°ƒç”¨æ–¹å¼
        """
        self.config = config
        self.query_id = query_id
        self.query_source = query_source
        self.save_context_snapshot = save_context_snapshot
        self.source_message = source_message

        # é˜¶æ®µä¸€é¢„å–ç¼“å­˜ï¼šé¿å…é˜¶æ®µäºŒé‡å¤æ‹‰å–/é‡å¤æ‹¼æ¥
        # ç»“æ„ï¼š{ code: {"df": <DataFrame>, "quote": <RealtimeQuote>} }
        self._prefetch_cache: Dict[str, Dict[str, Any]] = {}
        
        # === 1. é»˜è®¤é¡ºåºæ‰§è¡Œï¼ˆworkers=1ï¼‰ï¼Œé¿å…å¤šçº¿ç¨‹æ—¥å¿—äº¤é”™ ===
        if max_workers is None:
            max_workers = 1
            
        # === 2. åˆå§‹åŒ–å„ä¸ªæœåŠ¡ç»„ä»¶ ===
        self.fetcher_manager = DataFetcherManager()
        self.trend_analyzer = StockTrendAnalyzer()
        
        # åˆå§‹åŒ– LLM (ç›´æ¥ä» config è¯»å– key)
        self.analyzer = GeminiAnalyzer(api_key=config.gemini_api_key)
        
        # åˆå§‹åŒ– é€šçŸ¥æœåŠ¡
        self.notifier = NotificationService(source_message=source_message)
        
        # åˆå§‹åŒ– æ•°æ®åº“
        self.storage = DatabaseManager() 
        
        # === 3. åˆå§‹åŒ–æœç´¢æœåŠ¡ & æ™ºèƒ½æµæ§ ===
        self.search_service = None
        has_search_key = False
        
        # æ£€æŸ¥æ˜¯å¦é…ç½®äº†ä»»ä½•ä¸€ç§æœç´¢ Key
        if (config.bocha_api_keys or config.tavily_api_keys or 
            config.serpapi_keys or os.getenv("PERPLEXITY_API_KEY")):
            
            self.search_service = SearchService(
                bocha_keys=config.bocha_api_keys,
                tavily_keys=config.tavily_api_keys,
                serpapi_keys=config.serpapi_keys
            )
            has_search_key = True

        # å¦‚æœå¯ç”¨äº†æœç´¢ï¼Œå¼ºåˆ¶é™åˆ¶å¹¶å‘æ•°ï¼Œé˜²æ­¢ 429 é”™è¯¯
        if has_search_key:
            self.max_workers = min(max_workers, 2)
            logger.info(f"ğŸ•µï¸  [æ·±åº¦æ¨¡å¼] æœç´¢æœåŠ¡å·²å¯ç”¨ï¼Œå¹¶å‘é™åˆ¶ä¸º: {self.max_workers}")
        else:
            self.max_workers = max_workers
            logger.info(f"ğŸš€ [æé€Ÿæ¨¡å¼] çº¯æœ¬åœ°åˆ†æï¼Œå¹¶å‘æ•°: {self.max_workers}")

        # å¤§ç›˜ç›‘æ§ï¼šç”¨äºä¸ªè‚¡åˆ†ææ—¶çš„ã€Œä»“ä½ä¸Šé™/å‰ç½®æ»¤ç½‘ã€ï¼ˆå¤§ç›˜å®šä»“ä½ï¼Œä¸ªè‚¡å®šæ–¹å‘ï¼‰
        self._market_monitor = market_monitor
        if self._market_monitor:
            logger.info("ğŸ“Š [å¤§ç›˜ç›‘æ§] å·²å¯ç”¨ï¼Œä¸ªè‚¡åˆ†æå°†æ³¨å…¥å¤§ç›˜ç¯å¢ƒä½œä¸ºå‰ç½®æ»¤ç½‘")
        else:
            logger.warning("ğŸ“Š [å¤§ç›˜ç›‘æ§] æœªåŠ è½½ï¼Œä¸ªè‚¡åˆ†æå°†ä¸æ³¨å…¥å¤§ç›˜ç¯å¢ƒï¼ˆè¯·æ£€æŸ¥ data_provider.market_monitor ä¸ akshareï¼‰")

    def fetch_and_save_stock_data(self, code: str) -> (bool, str, Any, Any):
        """è·å–æ•°æ®å¹¶è½åº“ï¼Œä¿è¯ä¸‹æ¬¡å¯åšã€Œå†å²+å®æ—¶ã€æ‹¼æ¥ã€‚

        è¿”å›: (success, msg, df, quote)
        """
        try:
            # 120å¤©æ•°æ®ç”¨äºè®¡ç®—è¶‹åŠ¿ï¼ˆæœ‰å†å²åˆ™ DB+å®æ—¶ç¼åˆï¼Œæ— å†å²åˆ™å…¨é‡æŠ“å–ï¼‰
            df = self.fetcher_manager.get_merged_data(code, days=120)
            if df is None or df.empty:
                return False, "è·å–æ•°æ®ä¸ºç©º", None, None
            # å†™å…¥/æ›´æ–°æ—¥çº¿åˆ° DBï¼Œåç»­ run æ‰èƒ½ç”¨å†å²åšç¼åˆï¼ŒæŠ€æœ¯é¢æ‰å’Œç°å®ä¸€è‡´
            try:
                n = self.storage.save_daily_data(df, code, data_source="pipeline")
                if n > 0:
                    logger.debug(f"[{code}] æ—¥çº¿è½åº“æ–°å¢ {n} æ¡")
            except Exception as e:
                logger.warning(f"[{code}] æ—¥çº¿è½åº“å¤±è´¥(ç»§ç»­åˆ†æ): {e}")
            quote = self.fetcher_manager.get_realtime_quote(code)
            if not quote:
                return False, "å®æ—¶è¡Œæƒ…è·å–å¤±è´¥", df, None
            return True, "Success", df, quote
        except Exception as e:
            return False, str(e), None, None

    def _get_cached_news_context(self, code: str, stock_name: str, hours: int = 6, limit: int = 5) -> str:
        """ä¼˜å…ˆå¤ç”¨ news_intel ç¼“å­˜ï¼Œå‘½ä¸­åˆ™å‡å°‘å¤–éƒ¨æœç´¢ä¸ tokenã€‚"""
        try:
            items = self.storage.get_recent_news(code, days=1, limit=limit)
            if not items:
                return ""
            cutoff = datetime.now() - timedelta(hours=hours)
            fresh = [n for n in items if getattr(n, "fetched_at", None) and n.fetched_at >= cutoff]
            if not fresh:
                return ""
            lines = []
            for i, n in enumerate(fresh[:limit]):
                title = (getattr(n, "title", "") or "").strip()
                snippet = (getattr(n, "snippet", "") or "").strip()
                source = (getattr(n, "source", "") or "").strip()
                pub = getattr(n, "published_date", None)
                pub_str = f" ({pub})" if pub else ""
                head = f"{i+1}. ã€{source}ã€‘{title}{pub_str}".strip()
                body = snippet
                lines.append(f"{head}\n{body}".strip())
            return "\n".join(lines) if lines else ""
        except Exception:
            return ""

    def _prepare_stock_context(self, code: str) -> Optional[Dict[str, Any]]:
        """å‡†å¤‡ AI åˆ†ææ‰€éœ€çš„ä¸Šä¸‹æ–‡æ•°æ®"""
        prefetched = self._prefetch_cache.get(code) if hasattr(self, "_prefetch_cache") else None
        quote = (prefetched or {}).get("quote") or self.fetcher_manager.get_realtime_quote(code)
        if not quote:
            logger.warning(f"[{code}] æ— æ³•è·å–å®æ—¶è¡Œæƒ…ï¼Œè·³è¿‡")
            return None
        stock_name = quote.name
        
        try:
            cache_df = (prefetched or {}).get("df")
            if cache_df is not None:
                daily_df = cache_df
            else:
                daily_df = self.fetcher_manager.get_merged_data(code, days=120)
                # å•è‚¡/API è·¯å¾„æ—  prefetchï¼Œæ‹¿åˆ°æ•°æ®åè½åº“ï¼Œä¸‹æ¬¡åŒä¸€åªè‚¡å¯ç›´æ¥ç”¨ DB ç¼“å­˜
                if daily_df is not None and not daily_df.empty:
                    try:
                        self.storage.save_daily_data(daily_df, code, data_source="pipeline")
                    except Exception as e:
                        logger.debug(f"[{code}] æ—¥çº¿è½åº“å¤±è´¥(ç»§ç»­åˆ†æ): {e}")
        except Exception as e:
            logger.warning(f"[{code}] è·å–åˆå¹¶æ•°æ®å¤±è´¥: {e}")
            daily_df = None

        tech_report = "æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡ŒæŠ€æœ¯åˆ†æ"
        if daily_df is not None and not daily_df.empty:
            try:
                trend_result = self.trend_analyzer.analyze(daily_df, code)
                if quote.price:
                    trend_result.current_price = quote.price
                tech_report = self.trend_analyzer.format_analysis(trend_result)
            except Exception as e:
                logger.error(f"[{code}] æŠ€æœ¯åˆ†æç”Ÿæˆå¤±è´¥: {e}")

        # ç­¹ç æ•°æ®ï¼ˆå…ˆæŸ¥ DB/å†…å­˜ç¼“å­˜ï¼›å¤±è´¥æ—¶æ˜ç¡®æ ‡è®°ã€Œæš‚ä¸å¯ç”¨ã€é¿å…æ¨¡å‹çç¼–ï¼‰
        chip_data = {}
        chip_note = "æœªå¯ç”¨"
        if getattr(self.config, 'enable_chip_distribution', False) or getattr(self.config, 'chip_fetch_only_from_cache', False):
            chip = self.fetcher_manager.get_chip_distribution(code) if hasattr(self.fetcher_manager, 'get_chip_distribution') else None
            if chip:
                chip_data = chip.to_dict()
                chip_note = "è§ä¸‹æ•°æ®"
            else:
                chip_note = "æš‚ä¸å¯ç”¨ï¼ˆæ¥å£å¤±è´¥æˆ–æœªæ‹‰å–ï¼‰"
        
        # F10 åŸºæœ¬é¢æ•°æ®
        fundamental_data = {}
        try:
            fundamental_data = get_fundamental_data(code)
        except Exception as e:
            pass

        # å†å²è®°å¿†
        history_summary = None
        try:
            history_summary = self.storage.get_last_analysis_summary(code)
        except Exception as e:
            pass

        # å½“æ—¥/æ˜¨æ—¥ K çº¿ï¼ˆä¾›æ¨é€ä¸­çš„ã€Œå½“æ—¥è¡Œæƒ…ã€å¿«ç…§ç”¨ï¼‰
        today_row = {}
        yesterday_row = {}
        context_date = ''
        if daily_df is not None and not daily_df.empty and len(daily_df) >= 1:
            try:
                keys = ['open', 'high', 'low', 'close', 'volume', 'amount', 'pct_chg', 'date']
                last = daily_df.iloc[-1]
                today_row = {k: last[k] for k in keys if k in last.index}
                context_date = str(today_row.get('date', ''))
                if len(daily_df) >= 2:
                    prev = daily_df.iloc[-2]
                    yesterday_row = {k: prev[k] for k in keys if k in prev.index}
            except Exception:
                pass

        context = {
            'code': code,
            'stock_name': stock_name,
            'date': context_date,
            'today': today_row,
            'yesterday': yesterday_row,
            'price': quote.price,
            'realtime': quote.to_dict(),
            'chip': chip_data,
            'chip_note': chip_note,
            'technical_analysis_report': tech_report,
            'fundamental': fundamental_data,
            'history_summary': history_summary
        }
        return context

    def _log(self, msg: str, *args, **kwargs) -> None:
        """å¸¦ query_id çš„æ—¥å¿—å‰ç¼€ï¼Œä¾¿äºé“¾è·¯è¿½è¸ª"""
        prefix = f"[query_id={self.query_id}] " if self.query_id else ""
        logger.info(prefix + msg, *args, **kwargs)

    def process_single_stock(
        self,
        code: str,
        skip_analysis: bool = False,
        single_stock_notify: bool = False,
        report_type: ReportType = ReportType.SIMPLE,
        skip_data_fetch: bool = False,
        market_overview_override: Optional[str] = None,
    ) -> Optional[AnalysisResult]:
        """å¤„ç†å•åªè‚¡ç¥¨çš„æ ¸å¿ƒé€»è¾‘"""
        try:
            context = self._prepare_stock_context(code)
            if not context: return None
            stock_name = context['stock_name']
            self._log(f"[{code}] {stock_name} å¼€å§‹åˆ†æ")
            
            if skip_analysis:
                logger.info(f"[{code}] Dry-run æ¨¡å¼ï¼Œè·³è¿‡ AI åˆ†æ")
                return AnalysisResult(code=code, name=stock_name, reasoning="Dry Run æµ‹è¯•", operation_advice="è§‚æœ›", sentiment_score=50, trend_prediction="æµ‹è¯•", success=True)

            # === 1. æœç´¢èˆ†æƒ… (å¢åŠ éšæœºå»¶è¿Ÿé˜²å°å·) ===
            search_content = ""
            used_news_cache = False
            # 1) ä¼˜å…ˆå¤ç”¨ DB ç¼“å­˜ï¼ˆå‘½ä¸­åˆ™ä¸å¤–éƒ¨æœç´¢ã€ä¸ sleepï¼‰
            cached = self._get_cached_news_context(code, stock_name)
            if cached:
                search_content = cached
                used_news_cache = True
                logger.info(f"â™»ï¸  [{stock_name}] å‘½ä¸­èˆ†æƒ…ç¼“å­˜ï¼Œè·³è¿‡å¤–éƒ¨æœç´¢")
            # 2) æ— ç¼“å­˜å†èµ°å¤–éƒ¨æœç´¢
            elif self.search_service:
                # éšæœºä¼‘çœ  2.0 - 5.0 ç§’
                sleep_time = random.uniform(2.0, 5.0)
                time.sleep(sleep_time)
                
                logger.info(f"ğŸ” [{stock_name}] æ­£åœ¨ä¾¦æŸ¥èˆ†æƒ… (å»¶è¿Ÿ {sleep_time:.1f}s)...")
                try:
                    query = f"{stock_name} ({code}) è¿‘æœŸé‡å¤§åˆ©å¥½åˆ©ç©ºæ¶ˆæ¯ æœºæ„è§‚ç‚¹ ç ”æŠ¥"
                    if hasattr(self.search_service, 'search_stock_news'):
                        resp = self.search_service.search_stock_news(code, stock_name)
                    else:
                        resp = self.search_service.search(query)
                        
                    if resp and getattr(resp, 'success', False): 
                        search_content = resp.to_context()
                        # èˆ†æƒ…è½åº“ï¼Œä¾¿äºåç»­å¤ç”¨ä¸å®¡è®¡
                        if getattr(resp, 'results', None):
                            try:
                                self.storage.save_news_intel(
                                    code, stock_name, dimension="èˆ†æƒ…", query=query, response=resp,
                                    query_context={"query_id": self.query_id, "query_source": self.query_source}
                                )
                            except Exception as e:
                                logger.debug(f"[{stock_name}] èˆ†æƒ…è½åº“è·³è¿‡: {e}")
                except Exception as e:
                    logger.warning(f"[{stock_name}] æœç´¢æœåŠ¡å¼‚å¸¸: {e}")

            # === 2. è·å–å¤§ç›˜ç¯å¢ƒï¼ˆå‰ç½®æ»¤ç½‘ï¼šå¤§ç›˜å®šä»“ä½ä¸Šé™ï¼Œä¸ªè‚¡é€»è¾‘å®šä¹°å–æ–¹å‘ï¼‰===
            market_overview = market_overview_override
            if market_overview is None and self._market_monitor:
                try:
                    snapshot = self._market_monitor.get_market_snapshot()
                    if snapshot.get('success'):
                        vol = snapshot.get('total_volume', 'N/A')
                        indices = snapshot.get('indices', [])
                        idx_str = " / ".join([f"{i['name']} {i['change_pct']}%" for i in indices])
                        market_overview = f"ä»Šæ—¥ä¸¤å¸‚æˆäº¤é¢: {vol}äº¿ã€‚æŒ‡æ•°è¡¨ç°: {idx_str}ã€‚"
                        logger.info(f"ğŸ“Š [{stock_name}] å¤§ç›˜ç¯å¢ƒå·²æ³¨å…¥ï¼ˆæ»¤ç½‘ï¼‰: æˆäº¤é¢{vol}äº¿ | {idx_str}")
                except Exception as e:
                    logger.warning(f"[{stock_name}] è·å–å¤§ç›˜æ•°æ®å¾®ç‘•: {e}")

            self._log(f"ğŸ¤– [{stock_name}] è°ƒç”¨ LLM è¿›è¡Œåˆ†æ...")
            # æ— èˆ†æƒ…æ—¶ä¹Ÿç”¨è½»é‡æ¨¡å‹ï¼Œçœæˆæœ¬
            use_light = used_news_cache or (not search_content or not search_content.strip())
            # === 3. æ‰§è¡Œåˆ†æï¼ˆå¸¦è¶…æ—¶ï¼Œé»˜è®¤ 180 ç§’ï¼‰===
            analysis_timeout = getattr(self.config, 'analysis_timeout_seconds', 180) or 180
            def _run_analyze():
                return self.analyzer.analyze(
                    context=context,
                    news_context=search_content,
                    role="trader",
                    market_overview=market_overview,
                    use_light_model=use_light,
                )
            try:
                with ThreadPoolExecutor(max_workers=1) as ex:
                    fut = ex.submit(_run_analyze)
                    result = fut.result(timeout=analysis_timeout)
            except FuturesTimeoutError:
                logger.warning(f"[{stock_name}] åˆ†æè¶…æ—¶ ({analysis_timeout}s)ï¼Œè·³è¿‡")
                return None
            except Exception as e:
                logger.exception(f"[{stock_name}] åˆ†æå¼‚å¸¸: {e}")
                return None
            
            if not result: return None
            self._log(f"[åˆ†æå®Œæˆ] {stock_name}: å»ºè®®-{result.operation_advice}, è¯„åˆ†-{result.sentiment_score}")
            
            try:
                self.storage.save_analysis_history(result=result, query_id=self.query_id, report_type=report_type.value if hasattr(report_type, 'value') else str(report_type), news_content=search_content, context_snapshot=context if self.save_context_snapshot else None)
            except Exception as e:
                logger.error(f"ä¿å­˜åˆ†æå†å²å¤±è´¥: {e}")
            
            if single_stock_notify and self.notifier.is_available():
                try:
                    report = self.notifier.generate_single_stock_report(result)
                    self.notifier.send(report)
                except Exception as e:
                    logger.warning(f"[{code}] æ¨é€å¤±è´¥: {e}")
            return result
        except Exception as e:
            logger.exception(f"[{code}] å¤„ç†è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            return None

    def _send_notifications(self, results: List[AnalysisResult]):
        logger.info("æ­£åœ¨ç”Ÿæˆæ±‡æ€»æ—¥æŠ¥...")
        try:
            daily_report = self.notifier.generate_dashboard_report(results)
            self.notifier.send(daily_report)
            self.notifier.save_report_to_file(daily_report)
            # åŒæ—¶ä¿å­˜ä¸€ä»½ .txt åˆ°æœ¬åœ°ï¼Œä¸æ”¹å˜ PushPlus ç­‰æ¨é€é€»è¾‘
            from pathlib import Path
            reports_dir = Path(__file__).resolve().parents[2] / "reports"
            reports_dir.mkdir(parents=True, exist_ok=True)
            txt_name = f"report_{time.strftime('%Y%m%d')}.txt"
            txt_path = reports_dir / txt_name
            with open(txt_path, "w", encoding="utf-8") as f:
                f.write(daily_report)
            logger.info(f"æ—¥æŠ¥å·²ä¿å­˜ä¸º txt: {txt_path}")
        except Exception as e:
            logger.error(f"æ±‡æ€»æ¨é€å¤±è´¥: {e}")

    def run(self, stock_codes: Optional[List[str]] = None, dry_run: bool = False, send_notification: bool = True) -> List[AnalysisResult]:
        """
        ä¸»æ‰§è¡Œå…¥å£ (ç”± main.py è°ƒç”¨)
        """
        start_time = time.time()
        if stock_codes is None:
            self.config.refresh_stock_list()
            stock_codes = self.config.stock_list
        if not stock_codes:
            logger.error("æœªé…ç½®è‡ªé€‰è‚¡åˆ—è¡¨")
            return []
        
        total_stocks = len(stock_codes)
        logger.info(f"===== å¯åŠ¨åˆ†æä»»åŠ¡: å…± {total_stocks} åªè‚¡ç¥¨ =====")

        # === é˜¶æ®µä¸€ï¼šä¸²è¡Œè·å–æ•°æ® ===
        logger.info("ğŸ¢ é˜¶æ®µä¸€ï¼šä¸²è¡Œè·å–æ•°æ® (é˜²å°æ§ & é¢„åŠ è½½)...")
        valid_stocks = [] 
        
        for i, code in enumerate(stock_codes):
            try:
                success, msg, df, quote = self.fetch_and_save_stock_data(code)
                
                # å°è¯•é¢„å–ç­¹ç æ•°æ®ï¼ˆé¿å¼€äº¤æ˜“é«˜å³°ï¼‰
                try:
                    import datetime
                    now = datetime.datetime.now()
                    # ç®€å•åˆ¤æ–­éäº¤æ˜“æ—¶é—´æ‰å¤§é‡é¢„å–
                    is_trading = ((now.hour == 9 and now.minute >= 15) or (9 < now.hour < 15))
                    if not is_trading:
                        if hasattr(self.fetcher_manager, 'get_chip_distribution'):
                            self.fetcher_manager.get_chip_distribution(code)
                except Exception:
                    pass 

                if success:
                    valid_stocks.append(code)
                    # ç¼“å­˜é˜¶æ®µä¸€ç»“æœï¼Œé˜¶æ®µäºŒå¤ç”¨é¿å…é‡å¤å–æ•°/æ‹¼æ¥
                    if df is not None and quote is not None:
                        self._prefetch_cache[code] = {"df": df, "quote": quote}
                    logger.info(f"[{i+1}/{total_stocks}] âœ… {code} æ•°æ®å°±ç»ª")
                    # ä¸²è¡Œé˜¶æ®µä¹Ÿç¨å¾®ä¼‘æ¯ä¸€ä¸‹ï¼Œé˜²æ­¢æ•°æ®æºå°IP
                    if not dry_run:
                        time.sleep(0.5)
                else:
                    logger.warning(f"[{i+1}/{total_stocks}] âŒ {code} æ•°æ®å¤±è´¥: {msg}")
                
            except Exception as e:
                logger.error(f"[{code}] æ•°æ®é¢„å–å¼‚å¸¸: {e}")

        # === é˜¶æ®µäºŒï¼šå¹¶å‘åˆ†æ ===
        workers = self.max_workers if self.max_workers is not None else 1
        logger.info(f"ğŸ° é˜¶æ®µäºŒï¼šå¼€å¯ {workers} çº¿ç¨‹è¿›è¡Œ AI å¹¶å‘åˆ†æï¼ˆå¤šçº¿ç¨‹æ—¶æ—¥å¿—ä¼šäº¤é”™ï¼Œè‹¥éœ€é¡ºåºè¾“å‡ºè¯·ä½¿ç”¨ --workers 1ï¼‰...")
        single_stock_notify = getattr(self.config, 'single_stock_notify', False)
        report_type = ReportType.FULL if getattr(self.config, 'report_type', 'simple') == 'full' else ReportType.SIMPLE
        results: List[AnalysisResult] = []
        
        if not valid_stocks:
            logger.error("æ²¡æœ‰è·å–åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®ï¼Œç»ˆæ­¢åˆ†æ")
            return []

        # é˜¶æ®µäºŒï¼šå¤§ç›˜å¿«ç…§åªå–ä¸€æ¬¡ï¼ˆæ›´å¿«ã€æ›´ä¸€è‡´ï¼‰ï¼Œä¼ å…¥æ¯åªè‚¡ç¥¨
        market_overview_once: Optional[str] = None
        if self._market_monitor:
            try:
                snapshot = self._market_monitor.get_market_snapshot()
                if snapshot.get("success"):
                    vol = snapshot.get('total_volume', 'N/A')
                    indices = snapshot.get('indices', [])
                    idx_str = " / ".join([f"{i['name']} {i['change_pct']}%" for i in indices])
                    market_overview_once = f"ä»Šæ—¥ä¸¤å¸‚æˆäº¤é¢: {vol}äº¿ã€‚æŒ‡æ•°è¡¨ç°: {idx_str}ã€‚"
                    logger.info(f"ğŸ“Š [é˜¶æ®µäºŒ] å¤§ç›˜å¿«ç…§å·²è·å–ï¼ˆå…¨å±€å¤ç”¨ï¼‰: æˆäº¤é¢{vol}äº¿ | {idx_str}")
            except Exception as e:
                logger.warning(f"ğŸ“Š [é˜¶æ®µäºŒ] è·å–å¤§ç›˜å¿«ç…§å¤±è´¥(é™çº§ä¸ºé€è‚¡/ä¸æ³¨å…¥): {e}")

        with ThreadPoolExecutor(max_workers=workers) as executor:
            future_to_code = {
                executor.submit(
                    self.process_single_stock, 
                    code, 
                    skip_analysis=dry_run, 
                    single_stock_notify=single_stock_notify and send_notification, 
                    report_type=report_type, 
                    skip_data_fetch=True,
                    market_overview_override=market_overview_once
                ): code for code in valid_stocks
            }
            
            for future in as_completed(future_to_code):
                code = future_to_code[future]
                try:
                    res = future.result()
                    if res: results.append(res)
                except Exception as e:
                    logger.error(f"[{code}] AI åˆ†æä»»åŠ¡å¤±è´¥: {e}")
        
        logger.info(f"===== åˆ†æå®Œæˆï¼Œæ€»è€—æ—¶ {time.time() - start_time:.2f}s =====")
        
        # æ±‡æ€»æ¨é€ (å¦‚æœæ²¡å¼€å•è‚¡æ¨é€)
        if results and send_notification and not dry_run and not single_stock_notify:
            self._send_notifications(results)
            
        return results